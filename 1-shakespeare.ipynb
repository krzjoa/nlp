{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61707cb2",
   "metadata": {},
   "source": [
    "# Generating Shakespeare poetry with RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "06cd39b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torchtext\n",
    "# !pip3 install pytorch-lightning\n",
    "# !pip3 install keras-tcn --no-dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "913c2447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.transforms import VocabTransform\n",
    "# We only need TF to dowaload the file - an useful function is available in keras\n",
    "import tensorflow as tf\n",
    "from collections import OrderedDict\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# import ipdb\n",
    "\n",
    "# TF: https://www.tensorflow.org/text/tutorials/text_generation\n",
    "# https://www.kdnuggets.com/2020/07/pytorch-lstm-text-generation-tutorial.html\n",
    "# https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html\n",
    "# https://pytorch.org/docs/stable/quantization.html#torch.quantization.quantize_dynamic\n",
    "# https://anie.me/On-Torchtext/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4a3169",
   "metadata": {},
   "source": [
    "## Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b870271d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the file with Shakespare poetry\n",
    "path_to_file = tf.keras.utils.get_file(\n",
    "    'shakespeare.txt', \n",
    "    'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "99a995a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/krzysztof/.keras/datasets/shakespeare.txt'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "a83300a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open(path_to_file, 'rb').read()\n",
    "text = text.decode(encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ef95eb",
   "metadata": {},
   "source": [
    "## Unique tokens\n",
    "We assume the atomic token of the text is a letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2ae61ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = sorted(set(text))\n",
    "letters = OrderedDict([(i, 1) for i in letters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c50b5c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vocab(letters)\n",
    "vocab_transform = VocabTransform(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3cfd5013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocabulary.vocab.itos_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66d201a",
   "metadata": {},
   "source": [
    "## Splitting text by letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ba0714e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_text = list(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d3f3795f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = vocab_transform(splitted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97885d99",
   "metadata": {},
   "source": [
    "## Building a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "efebed39",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyShakespeare(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, n_letters, embedding_dim = 15, hidden_size = 32):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(n_letters, embedding_dim=embedding_dim)\n",
    "        self.rnn = nn.LSTM(input_size = embedding_dim, hidden_size = hidden_size, batch_first=True)\n",
    "        self.final_layer = nn.Linear(hidden_size, n_letters)\n",
    "        self.softmax = nn.Softmax(-1)\n",
    "        self.loss_fun = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, input):\n",
    "        emb = self.embed(input)\n",
    "        rnn_output, _ = self.rnn(emb)\n",
    "        last_vec = rnn_output[:,-1, :]\n",
    "        final_output = self.final_layer(last_vec) \n",
    "        return self.softmax(final_output)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self(x)\n",
    "        loss = self.loss_fun(output, y)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.parameters())\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "251f3666",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_shakespeare = TinyShakespeare(65)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "d4095a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting embedding's behaviour - it added a dimesnion at the end\n",
    "sample_input = torch.tensor([3, 10, 56]).reshape((1, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f5e46ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tiny_shakespeare(sample_input).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "7c86326d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoetryDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data, lookback, n_next, jump = 1):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "        self.lookback = lookback\n",
    "        self.n_next = n_next\n",
    "        self.jump = jump\n",
    "        self.length = len(data) // lookback+n_next\n",
    "        self.cardinality = np.unique(data).shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        lback = self.data[idx:(idx+self.lookback)]\n",
    "        nnext = self.data[(idx+self.lookback):(idx+self.lookback+1)]\n",
    "        \n",
    "        inp = torch.tensor(lback, dtype = torch.int).reshape((self.lookback))\n",
    "        \n",
    "        target = torch.zeros((65))\n",
    "        target[nnext] = 1\n",
    "        \n",
    "        return inp, target        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "2646a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd = PoetryDataset(encoded_text, lookback=10, n_next = 1)\n",
    "pdl = DataLoader(pd, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "09eca7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = pd[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "e67d32ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 65])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_shakespeare(X.reshape(-1, 10)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0d3810",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "e6784abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(accelerator='gpu', devices=1, max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6c96c54f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: /home/krzysztof/Pulpit/nlp/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name        | Type             | Params\n",
      "-------------------------------------------------\n",
      "0 | embed       | Embedding        | 975   \n",
      "1 | rnn         | LSTM             | 6.3 K \n",
      "2 | final_layer | Linear           | 2.1 K \n",
      "3 | softmax     | Softmax          | 0     \n",
      "4 | loss_fun    | CrossEntropyLoss | 0     \n",
      "-------------------------------------------------\n",
      "9.4 K     Trainable params\n",
      "0         Non-trainable params\n",
      "9.4 K     Total params\n",
      "0.038     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be83203d5d54718b66ebc811058766e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(model=tiny_shakespeare, train_dataloaders=pdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbcd1e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789728cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
